{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QuickDraw - 3 - Creating CoreML model \n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# The Quick, Draw! Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Quick Draw Dataset is a collection of 50 million drawings across 345 categories, contributed by players of the game Quick, Draw!. The drawings were captured as timestamped vectors, tagged with metadata including what the player was asked to draw and in which country the player was located. You can browse the recognized drawings on quickdraw.withgoogle.com/data or download the dataset from https://console.cloud.google.com/storage/browser/quickdraw_dataset/?pli=1.  \n",
    "\n",
    "The architecture was ported across from the tutorial <a href='https://www.tensorflow.org/versions/master/tutorials/recurrent_quickdraw'>Recurrent Neural Networks for Drawing Classification</a> (associated repo available <a href='https://github.com/tensorflow/models/tree/master/tutorials/rnn/quickdraw'>here</a>); of which many of the details have been used here.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://github.com/googlecreativelab/quickdraw-dataset/raw/master/preview.jpg'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.misc import imresize\n",
    "import os\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jnewnham/anaconda/envs/coreml27/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.0.8'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras \n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json \n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_DIR = '/Users/Joshua.Newnham/Documents/Data/quickdraw_dataset/sketchrnn_training_data/'\n",
    "\n",
    "TRAINING_PARTS = 5 \n",
    "\n",
    "MAX_SEQ_LEN = 75\n",
    "CLASSES = 172"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_stroke_sequence(x, max_len=MAX_SEQ_LEN):\n",
    "    padded_x = np.zeros((x.shape[0], max_len, 3), dtype=np.float32)\n",
    "    for i in range(x.shape[0]):\n",
    "        X = x[i]\n",
    "        if X.shape[0] > max_len:\n",
    "            X = X[:max_len, :]\n",
    "        elif X.shape[0] < max_len:\n",
    "            padding = np.array([[0,0,0]] * (max_len-X.shape[0]), dtype=np.float32)            \n",
    "            X = np.vstack((padding, X))\n",
    "            \n",
    "        padded_x[i] = X\n",
    "        \n",
    "    return padded_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load files \n",
    "\n",
    "train_x_files = [] \n",
    "train_y_files = []\n",
    "valid_x_files = [] \n",
    "valid_y_files = []\n",
    "\n",
    "for part_num in range(1, TRAINING_PARTS+1):\n",
    "    train_x_files.append(os.path.join(DATASET_DIR, \"train_{}_x.npy\".format(part_num)))\n",
    "    train_y_files.append(os.path.join(DATASET_DIR, \"train_{}_y.npy\".format(part_num)))\n",
    "    valid_x_files.append(os.path.join(DATASET_DIR, \"validation_{}_x.npy\".format(part_num)))\n",
    "    valid_y_files.append(os.path.join(DATASET_DIR, \"validation_{}_y.npy\".format(part_num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_1_x (344000,), train_1_y (344000, 172), padded_train_1_x (344000, 75, 3)\n",
      "train_1_y (34400,), valid_1_y (34400, 172), padded_valid_1_x (34400, 75, 3)\n"
     ]
    }
   ],
   "source": [
    "train_1_x = np.load(train_x_files[0])\n",
    "train_1_y = np.load(train_y_files[0])\n",
    "valid_1_x = np.load(valid_x_files[0])\n",
    "valid_1_y = np.load(valid_y_files[0])\n",
    "\n",
    "padded_train_1_x = pad_stroke_sequence(train_1_x)\n",
    "padded_valid_1_x = pad_stroke_sequence(valid_1_x)\n",
    "\n",
    "print(\"train_1_x {}, train_1_y {}, padded_train_1_x {}\\ntrain_1_y {}, valid_1_y {}, padded_valid_1_x {}\".format(\n",
    "    train_1_x.shape, \n",
    "    train_1_y.shape, \n",
    "    padded_train_1_x.shape, \n",
    "    valid_1_x.shape, \n",
    "    valid_1_y.shape, \n",
    "    padded_valid_1_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and model weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('output/quickdraw_arch.json', 'r') as f:\n",
    "    json_string = json.load(f)\n",
    "    model = model_from_json(json_string)\n",
    "    \n",
    "model.load_weights('output/quickdraw_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_0 (Conv1D)            (None, 75, 48)            768       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 75, 48)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 75, 64)            15424     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 75, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 75, 96)            18528     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 75, 96)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 75, 128)           36992     \n",
      "_________________________________________________________________\n",
      "lstm_0 (Bidirectional)       (None, 75, 256)           263168    \n",
      "_________________________________________________________________\n",
      "lstm_1 (Bidirectional)       (None, 75, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_2 (Bidirectional)       (None, 75, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 19200)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 172)               3302572   \n",
      "=================================================================\n",
      "Total params: 4,425,932\n",
      "Trainable params: 4,425,932\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test our model by making some predictions on our loaded training and validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded categories - number of categories 172\n"
     ]
    }
   ],
   "source": [
    "### Load categories \n",
    "categories = {}\n",
    "\n",
    "with open('labels.csv', 'r') as f:\n",
    "    csv_reader = csv.reader(f)\n",
    "    for row in csv_reader:\n",
    "        categories[int(row[0])] = row[1]\n",
    "        \n",
    "print(\"Loaded categories - number of categories {}\".format(len(categories)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Sort by key \n",
    "categories = [categories[key] for key in sorted(categories.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_prediction(model, labels, X, index):    \n",
    "    x = X[index]\n",
    "    x = x.reshape(1, \n",
    "                  x.shape[0], \n",
    "                  x.shape[1])\n",
    "    x = pad_stroke_sequence(x)\n",
    "    \n",
    "    prediction = np.argmax(\n",
    "        model.predict(x)\n",
    "    )\n",
    "    return labels[prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction airplane expected airplane\n"
     ]
    }
   ],
   "source": [
    "prediction = make_prediction(model, categories, train_1_x, 0)\n",
    "print(\"prediction {} expected {}\".format(prediction, categories[np.argmax(train_1_y[0])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction banana expected banana\n"
     ]
    }
   ],
   "source": [
    "prediction = make_prediction(model, categories, valid_1_x, 1700)\n",
    "print(\"prediction {} expected {}\".format(prediction, categories[np.argmax(valid_1_y[1700])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25496 :: prediction school bus expected school bus\n"
     ]
    }
   ],
   "source": [
    "idx = random.randint(0, valid_1_x.size)\n",
    "prediction = make_prediction(model, categories, valid_1_x, idx)\n",
    "print(\"{} :: prediction {} expected {}\".format(idx, \n",
    "                                               prediction, \n",
    "                                               categories[np.argmax(valid_1_y[idx])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22323 :: prediction parrot expected parrot\n"
     ]
    }
   ],
   "source": [
    "idx = random.randint(0, valid_1_x.size)\n",
    "prediction = make_prediction(model, categories, valid_1_x, idx)\n",
    "print(\"{} :: prediction {} expected {}\".format(idx, \n",
    "                                               prediction, \n",
    "                                               categories[np.argmax(valid_1_y[idx])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CoreML model\n",
    "\n",
    "https://apple.github.io/coremltools/index.html  \n",
    "https://www.pydoc.io/pypi/coremltools-0.7/autoapi/converters/keras/_layers2/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, rather than loading the arhcitecture, we create our model again with a minor adjustment in that we modify the expected input to be a flatten vector from 75,3 to 255. The reason for this is that CoreML (the converter) assumes the input shape is [Seq, D], and therefore returns (dim[1],) where dim would be (75,3) in your case i.e. will have the input shape as [3].  \n",
    "We get around this by flattening (75,3) to (255) and reshape it before feeding it into our previous architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful information about working with sequnece data can be found on the Apple developer website https://developer.apple.com/documentation/coreml/core_ml_api/making_predictions_with_a_sequence_of_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import coremltools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from keras import preprocessing\n",
    "from keras.preprocessing import image\n",
    "\n",
    "BATCH_SIZE = 512 \n",
    "MAX_SEQ_LEN = 75\n",
    "CLASSES = 172\n",
    "NUM_RNN_LAYERS = 3 \n",
    "NUM_RNN_NODES = 128\n",
    "NUM_CONV = [48, 64, 96, 128]\n",
    "CONV_LEN = [5, 5, 3, 3]\n",
    "DROPOUT = 0.3\n",
    "\n",
    "def create_model(input_shape=(MAX_SEQ_LEN, 3), \n",
    "                 num_conv=NUM_CONV, \n",
    "                 conv_len=CONV_LEN, \n",
    "                 dropout=DROPOUT, \n",
    "                 batch_size=BATCH_SIZE, \n",
    "                 num_rnn_layers=NUM_RNN_LAYERS, \n",
    "                 num_rnn_nodes=NUM_RNN_NODES, \n",
    "                 num_classes=CLASSES):\n",
    "    \n",
    "    model = models.Sequential() \n",
    "    \n",
    "    model.add(layers.Reshape(input_shape, input_shape=(input_shape[0] * input_shape[1],)))\n",
    "    \n",
    "    for i, filters in enumerate(num_conv):\n",
    "        if i == 0:\n",
    "            # TODO: feasible to use a TimeDistributed wrapper here? https://keras.io/layers/wrappers/\n",
    "            model.add(\n",
    "                layers.Conv1D(filters=filters, \n",
    "                              kernel_size=conv_len[i], \n",
    "                              activation=None, \n",
    "                              strides=1, \n",
    "                              padding='same', \n",
    "                              name='conv1d_{}'.format(i)))\n",
    "        else:\n",
    "            model.add(layers.Dropout(dropout, name=\"dropout_{}\".format(i)))\n",
    "            model.add(layers.Conv1D(filters=filters, \n",
    "                                    kernel_size=conv_len[i], \n",
    "                                    activation=None, \n",
    "                                    strides=1, \n",
    "                                    padding='same', \n",
    "                                    name='conv1d_{}'.format(i)))\n",
    "      \n",
    "    for i in range(num_rnn_layers):\n",
    "        model.add(layers.Bidirectional(layers.LSTM(units=num_rnn_nodes, \n",
    "                                                   return_sequences=True, \n",
    "                                                   recurrent_dropout=dropout), \n",
    "                                       name=\"lstm_{}\".format(i)))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
    "    \n",
    "                      \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_1 (Reshape)          (None, 75, 3)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_0 (Conv1D)            (None, 75, 48)            768       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 75, 48)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 75, 64)            15424     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 75, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 75, 96)            18528     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 75, 96)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 75, 128)           36992     \n",
      "_________________________________________________________________\n",
      "lstm_0 (Bidirectional)       (None, 75, 256)           263168    \n",
      "_________________________________________________________________\n",
      "lstm_1 (Bidirectional)       (None, 75, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_2 (Bidirectional)       (None, 75, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 19200)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 172)               3302572   \n",
      "=================================================================\n",
      "Total params: 4,425,932\n",
      "Trainable params: 4,425,932\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.load_weights('output/quickdraw_weights.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_prediction(model, labels, X, index): \n",
    "    \"\"\"\n",
    "    Modified prediction function that flattens the data for our CoreML model \n",
    "    (sanity check that everything is working correctly)\n",
    "    \"\"\"\n",
    "    x = X[index]\n",
    "    x = x.reshape(1, \n",
    "                  x.shape[0], \n",
    "                  x.shape[1])\n",
    "    x = pad_stroke_sequence(x)\n",
    "    \n",
    "    prediction = np.argmax(\n",
    "        model.predict(x.reshape(1, -1))\n",
    "    )\n",
    "    return labels[prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction airplane, actual airplane\n"
     ]
    }
   ],
   "source": [
    "print(\"prediction {}, actual {}\".format(\n",
    "    make_prediction(model, categories, train_1_x, 0), \n",
    "    categories[np.argmax(train_1_y[0])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction apple, actual apple\n"
     ]
    }
   ],
   "source": [
    "print(\"prediction {}, actual {}\".format(\n",
    "    make_prediction(model, categories, train_1_x, 8000), \n",
    "    categories[np.argmax(train_1_y[8000])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction barn, actual barn\n"
     ]
    }
   ],
   "source": [
    "print(\"prediction {}, actual {}\".format(\n",
    "    make_prediction(model, categories, train_1_x, 18000), \n",
    "    categories[np.argmax(train_1_y[18000])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : reshape_1_input, <keras.engine.topology.InputLayer object at 0x137090a50>\n",
      "1 : reshape_1, <keras.layers.core.Reshape object at 0x137090a10>\n",
      "2 : reshape_1_permute_conv1d_0, <keras.layers.core.Permute object at 0x139bca290>\n",
      "3 : conv1d_0, <keras.layers.convolutional.Conv1D object at 0x137090d10>\n",
      "4 : conv1d_1, <keras.layers.convolutional.Conv1D object at 0x137090e50>\n",
      "5 : conv1d_2, <keras.layers.convolutional.Conv1D object at 0x137d51f10>\n",
      "6 : conv1d_3, <keras.layers.convolutional.Conv1D object at 0x136366fd0>\n",
      "7 : conv1d_3_permute_lstm_0, <keras.layers.core.Permute object at 0x136c4c250>\n",
      "8 : lstm_0, <keras.layers.wrappers.Bidirectional object at 0x1352c1350>\n",
      "9 : lstm_1, <keras.layers.wrappers.Bidirectional object at 0x12d84cd10>\n",
      "10 : lstm_2, <keras.layers.wrappers.Bidirectional object at 0x135364c10>\n",
      "11 : flatten_1, <keras.layers.core.Flatten object at 0x1370b4c10>\n",
      "12 : dense_1, <keras.layers.core.Dense object at 0x13b00d190>\n",
      "13 : dense_1__activation__, <keras.layers.core.Activation object at 0x136c4c2d0>\n"
     ]
    }
   ],
   "source": [
    "coreml_model = coremltools.converters.keras.convert(model, \n",
    "                                                    input_names=['strokeSeq'],\n",
    "                                                    output_names=['classLabelProbs'],\n",
    "                                                    class_labels=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coreml_model.author = 'Joshua Newnham'\n",
    "coreml_model.license = 'BSD'\n",
    "coreml_model.short_description = 'Freehand sketch recognizer trained using the dataset from Googles AI experiment AutoDraw using their QuickDraw dataset'\n",
    "coreml_model.input_description['strokeSeq'] = 'Sequence of strokes - flattened (75,3) to (255)'\n",
    "coreml_model.output_description['classLabelProbs'] = 'Probability of each category (Dict where the key is the category and value is the probability)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coreml_model.save('output/quickdraw.mlmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test our CoreML model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_coreml(coreml_model, \n",
    "                labels, \n",
    "                X, \n",
    "                Y,\n",
    "                index):\n",
    "    \n",
    "    x = X[index]\n",
    "    x = x.reshape(1, \n",
    "                  x.shape[0], \n",
    "                  x.shape[1])\n",
    "    x = pad_stroke_sequence(x)\n",
    "    \n",
    "    x = X[index]\n",
    "    x = x.reshape(1, \n",
    "                  x.shape[0], \n",
    "                  x.shape[1])\n",
    "    x = pad_stroke_sequence(x)\n",
    "    \n",
    "    results = coreml_model.predict({'strokeSeq':x.reshape(-1)})\n",
    "    predicted_prob = results['classLabelProbs']\n",
    "    return sorted(predicted_prob.items(), key=lambda (k,v): (v,k), reverse=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index :: 0 predicted (u'airplane', 0.8395060300827026) actual airplane\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print(\"index :: {} predicted {} actual {}\".format(\n",
    "    idx, \n",
    "    test_coreml(coreml_model, categories, train_1_x, train_1_y, idx), \n",
    "    categories[np.argmax(train_1_y[idx])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index :: 138468 predicted (u'hat', 0.7071692943572998) actual hat\n"
     ]
    }
   ],
   "source": [
    "idx = random.randint(0, train_1_x.size)\n",
    "print(\"index :: {} predicted {} actual {}\".format(\n",
    "    idx, \n",
    "    test_coreml(coreml_model, categories, train_1_x, train_1_y, idx), \n",
    "    categories[np.argmax(train_1_y[idx])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index :: 293140 predicted (u'submarine', 0.6821426153182983) actual submarine\n"
     ]
    }
   ],
   "source": [
    "idx = random.randint(0, train_1_x.size)\n",
    "print(\"index :: {} predicted {} actual {}\".format(\n",
    "    idx, \n",
    "    test_coreml(coreml_model, categories, train_1_x, train_1_y, idx), \n",
    "    categories[np.argmax(train_1_y[idx])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
