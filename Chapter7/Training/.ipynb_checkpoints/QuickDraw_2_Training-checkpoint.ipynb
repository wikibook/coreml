{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Quick, Draw! Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Quick Draw Dataset is a collection of 50 million drawings across 345 categories, contributed by players of the game Quick, Draw!. The drawings were captured as timestamped vectors, tagged with metadata including what the player was asked to draw and in which country the player was located. You can browse the recognized drawings on quickdraw.withgoogle.com/data or download the dataset from https://console.cloud.google.com/storage/browser/quickdraw_dataset/?pli=1.  \n",
    "\n",
    "The architecture was ported across from the tutorial <a href='https://www.tensorflow.org/versions/master/tutorials/recurrent_quickdraw'>Recurrent Neural Networks for Drawing Classification</a> (associated repo available <a href='https://github.com/tensorflow/models/tree/master/tutorials/rnn/quickdraw'>here</a>); of which many of the details have been used here.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://github.com/googlecreativelab/quickdraw-dataset/raw/master/preview.jpg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.misc import imresize\n",
    "import os\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from keras import preprocessing\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_DIR = '/Users/Joshua.Newnham/Documents/Data/quickdraw_dataset/sketchrnn_training_data/'\n",
    "TRAINING_PARTS = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 8 \n",
    "MAX_SEQ_LEN = 80\n",
    "CLASSES = 172\n",
    "NUM_RNN_LAYERS = 3 \n",
    "NUM_RNN_NODES = 128\n",
    "NUM_CONV = [48, 64, 96]\n",
    "CONV_LEN = [5, 5, 3]\n",
    "DROPOUT = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_accuracy_loss(history):\n",
    "    acc = history['acc']\n",
    "    val_acc = history['val_acc']\n",
    "    loss = history['loss']\n",
    "    val_loss = history['val_loss']\n",
    "\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch(x, y, batch_size=BATCH_SIZE):\n",
    "    return x.reshape(batch_size, -1, 3), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_stroke_sequence(x, max_len=MAX_SEQ_LEN):\n",
    "    padded_x = np.zeros((x.shape[0], max_len, 3), dtype=np.float32)\n",
    "    for i in range(x.shape[0]):\n",
    "        X = x[i]\n",
    "        if X.shape[0] > max_len:\n",
    "            X = X[:max_len, :]\n",
    "        elif X.shape[0] < max_len:\n",
    "            padding = np.array([[0,0,0]] * (max_len-X.shape[0]), dtype=np.float32)            \n",
    "            X = np.vstack((padding, X))\n",
    "            \n",
    "        padded_x[i] = X\n",
    "        \n",
    "    return padded_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(input_shape=(MAX_SEQ_LEN, 3), \n",
    "                 num_conv=NUM_CONV, \n",
    "                 conv_len=CONV_LEN, \n",
    "                 dropout=DROPOUT, \n",
    "                 batch_size=BATCH_SIZE, \n",
    "                 num_rnn_layers=NUM_RNN_LAYERS, \n",
    "                 num_rnn_nodes=NUM_RNN_NODES, \n",
    "                 num_classes=CLASSES):\n",
    "    \n",
    "    model = models.Sequential() \n",
    "    for i, filters in enumerate(num_conv):\n",
    "        if i == 0:\n",
    "            # TODO: feasible to use a TimeDistributed wrapper here? https://keras.io/layers/wrappers/\n",
    "            model.add(\n",
    "                layers.Conv1D(filters=filters, \n",
    "                              kernel_size=conv_len[i], \n",
    "                              activation=None, \n",
    "                              strides=1, \n",
    "                              padding='same', \n",
    "                              name='conv1d_{}'.format(i), input_shape=input_shape))\n",
    "        else:\n",
    "            model.add(layers.Dropout(dropout, name=\"dropout_{}\".format(i)))\n",
    "            model.add(layers.Conv1D(filters=filters, \n",
    "                                    kernel_size=conv_len[i], \n",
    "                                    activation=None, \n",
    "                                    strides=1, \n",
    "                                    padding='same', \n",
    "                                    name='conv1d_{}'.format(i)))\n",
    "      \n",
    "    for i in range(num_rnn_layers):\n",
    "        model.add(layers.Bidirectional(layers.LSTM(units=num_rnn_nodes, \n",
    "                                                   return_sequences=True, \n",
    "                                                   recurrent_dropout=dropout), \n",
    "                                       name=\"lstm_{}\".format(i)))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
    "    \n",
    "                      \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, \n",
    "          train_x_files, train_y_files, valid_x_files, valid_y_files, \n",
    "          batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "          max_seq_len=MAX_SEQ_LEN, \n",
    "          load_previous_weights=True, model_weights_file=\"output/quickdraw_weights.h5\"):\n",
    "    \n",
    "    # load previous weights (if applicable)\n",
    "    if model_weights_file is not None and os.path.isfile(model_weights_file) and load_previous_weights:\n",
    "        print(\"Loading weights from file {}\".format(model_weights_file))\n",
    "        model.load_weights(model_weights_file)\n",
    "    \n",
    "    checkpoint = callbacks.ModelCheckpoint(model_weights_file, \n",
    "                                           monitor='val_loss', \n",
    "                                           verbose=0, \n",
    "                                           save_best_only=True, \n",
    "                                           save_weights_only=True, \n",
    "                                           mode='auto', \n",
    "                                           period=1)\n",
    "    \n",
    "    # compile model \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer='rmsprop', \n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    history_file = \"{}_history.pickle\".format(model_weights_file.replace(\".h5\", \"\"))\n",
    "    \n",
    "    if os.path.isfile(history_file):\n",
    "        with open(history_file, 'rb') as f:\n",
    "            accumulated_history = pickle.load(f)\n",
    "    else:\n",
    "        accumulated_history = {\n",
    "            'acc': [], \n",
    "            'val_acc': [], \n",
    "            'loss': [], \n",
    "            'val_loss': []\n",
    "        }                        \n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(len(train_x_files)):\n",
    "            # load data for this iteration \n",
    "            train_x = np.load(train_x_files[i])\n",
    "            train_y = np.load(train_y_files[i])\n",
    "            \n",
    "            valid_x = np.load(valid_x_files[i])\n",
    "            valid_y = np.load(valid_y_files[i])\n",
    "    \n",
    "            # prepare training and validation data \n",
    "            train_x = pad_stroke_sequence(train_x)\n",
    "            valid_x = pad_stroke_sequence(valid_x)        \n",
    "    \n",
    "            history = model.fit(train_x, train_y,\n",
    "                                batch_size=batch_size, \n",
    "                                epochs=1,\n",
    "                                validation_data=(valid_x, valid_y), \n",
    "                                shuffle=True, \n",
    "                                callbacks=[checkpoint])\n",
    "        \n",
    "            accumulated_history['acc'] += history.history['acc']\n",
    "            accumulated_history['val_acc'] = history.history['val_acc']\n",
    "            accumulated_history['loss'] = history.history['loss']\n",
    "            accumulated_history['val_loss'] = history.history['val_loss'] \n",
    "            \n",
    "            with open(history_file, 'wb') as f:\n",
    "                pickle.dump(accumulated_history, f)\n",
    "    \n",
    "    return model, accumulated_history "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load files \n",
    "\n",
    "train_x_files = [] \n",
    "train_y_files = []\n",
    "valid_x_files = [] \n",
    "valid_y_files = []\n",
    "\n",
    "for part_num in range(1, TRAINING_PARTS+1):\n",
    "    train_x_files.append(os.path.join(DATASET_DIR, \"train_{}_x.npy\".format(part_num)))\n",
    "    train_y_files.append(os.path.join(DATASET_DIR, \"train_{}_y.npy\".format(part_num)))\n",
    "    valid_x_files.append(os.path.join(DATASET_DIR, \"validation_{}_x.npy\".format(part_num)))\n",
    "    valid_y_files.append(os.path.join(DATASET_DIR, \"validation_{}_y.npy\".format(part_num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1_x = np.load(train_x_files[0])\n",
    "train_1_y = np.load(train_y_files[0])\n",
    "padded_train_1_x = pad_stroke_sequence(train_1_x)\n",
    "\n",
    "print(\"train_1_x {}, train_1_y {}, padded_train_1_x {}\".format(\n",
    "    train_1_x.shape, \n",
    "    train_1_y.shape, \n",
    "    padded_train_1_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_1_x = np.load(valid_x_files[0])\n",
    "valid_1_y = np.load(valid_y_files[0])\n",
    "padded_valid_1_x = pad_stroke_sequence(valid_1_x)\n",
    "\n",
    "print(\"valid_1_x {}, valid_1_y {}, padded_valid_1_x {}\".format(\n",
    "    valid_1_x.shape, \n",
    "    valid_1_y.shape, \n",
    "    padded_valid_1_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model and train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, training_history = train(model, \n",
    "                                train_x_files=train_x_files, \n",
    "                                train_y_files=train_y_files, \n",
    "                                valid_x_files=valid_x_files, \n",
    "                                valid_y_files=valid_y_files, \n",
    "                                load_previous_weights=True, \n",
    "                                model_weights_file=\"output/quickdraw_weights_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_accuracy_loss(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
